{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import random module\n",
    "import random\n",
    "\n",
    "# import torch modules\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "#import numpy and matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#set manual seed for torch and random module\n",
    "manualSeed = 999\n",
    "print(\"Random Seed: \", manualSeed)\n",
    "random.seed(manualSeed)\n",
    "torch.manual_seed(manualSeed)\n",
    "\n",
    "# some images files get truncated when converted from image to tensor format\n",
    "# To avoid error set the load truncated images True\n",
    "from PIL import Image, ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# location of dataset images prepared for training\n",
    "dataroot = \"Dataset_location\"\n",
    "\n",
    "# number of worker is the number of threads for loading the images\n",
    "workers = 4\n",
    "\n",
    "# set ngpu=1 to use GPU for training or to 0 otherwise\n",
    "ngpu = 1\n",
    "\n",
    "#set the device to gpu or cpu \n",
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "#uncomment and print to view the details of avaliable GPU and avaliable memory\n",
    "# torch.cuda.get_device_properties(device)\n",
    "# torch.cuda.max_memory_allocated(device=device)\n",
    "\n",
    "# set batch size based on GPU memory avaliable\n",
    "batch_size = 12\n",
    "\n",
    "# image size and number of channels\n",
    "image_size = 1536\n",
    "nc = 3\n",
    "\n",
    "# shape of noise vector\n",
    "nz = 100\n",
    "\n",
    "# number of base feature map in generator and discrimininator\n",
    "ngf = 32\n",
    "ndf = 32\n",
    "\n",
    "# set num_epochs based on batch size and total number of images avaliable\n",
    "num_epochs = 1666\n",
    "\n",
    "# torchvision dataset is used to pre load all the images, normalize and to converted to tensors\n",
    "dataset = dset.ImageFolder(root=dataroot,\n",
    "                           transform=transforms.Compose([\n",
    "                               transforms.Resize(image_size),\n",
    "                               transforms.CenterCrop(image_size),\n",
    "                               transforms.ToTensor(),\n",
    "                               transforms.Normalize((0.1, 0.1, 0.1), (0.1, 0.1, 0.1)),\n",
    "                           ]))\n",
    "# torchvision dataloader is used shuffle the images  and to wrap an iterable for trainig based on batch size\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
    "                                         shuffle=True, num_workers=workers)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to intialize weights of the generator and discriminator \n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the parameter for generator G \n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Generator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.main = nn.Sequential(\n",
    "          # Layer 1\n",
    "            #nn.ConvTranspose2d(input features, output features, kernel size, stride, padding, enable or disable bias),\n",
    "            nn.ConvTranspose2d(nz, ngf * 32, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 32), # batch normalization layer \n",
    "            nn.SELU(inplace=True), # activation layer \n",
    "            # Layer 2\n",
    "            nn.ConvTranspose2d(ngf * 32 , ngf * 16, 4, 3, 0, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 16),\n",
    "            nn.SELU(inplace=True),\n",
    "            # Layer 3\n",
    "            nn.ConvTranspose2d(ngf * 16, ngf * 8, 4, 3, 0, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 8 ),\n",
    "            nn.SELU(inplace=True),\n",
    "            # Layer 4\n",
    "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 3, 0, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.SELU(inplace=True),\n",
    "            # Layer 5\n",
    "            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 3, 0, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.SELU(inplace=True),\n",
    "            # Layer 6\n",
    "            nn.ConvTranspose2d(ngf * 2, ngf , 4, 2, 0, bias=False),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.SELU(inplace=True),\n",
    "            # Layer 7 \n",
    "            nn.ConvTranspose2d( ngf, nc, 4, 2, 0, bias=False),\n",
    "            nn.Tanh()   # tanh activation in the last layer \n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load generator G into GPU memory \n",
    "netG = Generator(ngpu).to(device)\n",
    "if (device.type == 'cuda') and (ngpu > 1):\n",
    "    netG = nn.DataParallel(netG, list(range(ngpu)))\n",
    "#intialize weights for generator G\n",
    "netG.apply(weights_init)\n",
    "#print model \n",
    "print(netG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the parameter for discriminator D\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.main = nn.Sequential(\n",
    "            # Layer 1\n",
    "            #nn.Conv2d(input features, output features, kernel size, stride, padding, enable or disable bias),\n",
    "            nn.Conv2d(nc, ndf, 4, 2, 0, bias=False),\n",
    "            nn.SELU(inplace=True), # by enabling inplace parameter the tensors are changed directly without making a copy\n",
    "            # Layer 2\n",
    "            nn.Conv2d(ndf, ndf * 2, 4, 2, 0, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.SELU(inplace=True),\n",
    "            # Layer 3\n",
    "            nn.Conv2d(ndf * 2, ndf * 4, 4, 3, 0, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.SELU(inplace=True),\n",
    "            # Layer 4\n",
    "            nn.Conv2d(ndf * 4, ndf * 8, 4, 3, 0, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 8),\n",
    "            nn.SELU(inplace=True),\n",
    "            # Layer 5\n",
    "            nn.Conv2d(ndf * 8, ndf * 16, 4, 3, 0, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 16),\n",
    "            nn.SELU(inplace=True),\n",
    "            # Layer 6\n",
    "            nn.Conv2d(ndf * 16, ndf * 32, 4, 3, 0, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 32),\n",
    "            nn.SELU(inplace=True),\n",
    "            # Layer 7\n",
    "            nn.Conv2d(ndf * 32, 1, 4, 1, 0, bias=False),\n",
    "            nn.Sigmoid() # sigmoid activation in the final layer\n",
    "            \n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the discriminator into GPU memory\n",
    "netD = Discriminator(ngpu).to(device)\n",
    "if (device.type == 'cuda') and (ngpu > 1):\n",
    "    netD = nn.DataParallel(netD, list(range(ngpu)))\n",
    "#intilize the weights for discriminator\n",
    "netD.apply(weights_init)\n",
    "#print the model\n",
    "print(netD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the hyperparameters for training\n",
    "\n",
    "#generator  and discriminator learning rate\n",
    "lrG = 0.00025\n",
    "lrD = 0.00015\n",
    "\n",
    "# setup adam optimizer\n",
    "beta1 = 0.5\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=lrD, betas=(beta1, 0.999))\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=lrG, betas=(beta1, 0.999))\n",
    "\n",
    "#initialize  loss function \n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# initialize loss vector\n",
    "fixed_noise = torch.randn(1, nz, 1, 1, device=device)\n",
    "\n",
    "# labelling for real and fake images\n",
    "real_label = 0.9\n",
    "fake_label = 0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to plot the gradient flow in generator during trainig\n",
    "def plot_grad_flow(named_parameters):\n",
    "    ave_grads = []\n",
    "    layers = []\n",
    "    for n, p in named_parameters:\n",
    "        if(p.requires_grad) and (\"bias\" not in n):\n",
    "            layers.append(n)\n",
    "            ave_grads.append(p.grad.abs().mean().cpu().detach().numpy())\n",
    "    plt.plot(ave_grads, alpha=0.3, color=\"b\")\n",
    "    plt.hlines(0, 0, len(ave_grads)+1, linewidth=1, color=\"k\" )\n",
    "    plt.xticks(range(0,len(ave_grads), 1), layers, rotation=\"vertical\")\n",
    "    plt.xlim(xmin=0, xmax=len(ave_grads))\n",
    "    plt.xlabel(\"Layers\")\n",
    "    plt.ylabel(\"average gradient\")\n",
    "    plt.title(\"Gradient flow\")\n",
    "    plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list to record generator and discriminator losses during training\n",
    "G_losses = []\n",
    "D_losses = []\n",
    "iters = 0\n",
    "\n",
    "# iterate from 0 to number of epochs\n",
    "for epoch in range(num_epochs):\n",
    "    # iterate through the dataloader \n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "\n",
    "        #----------------------Discriminator Training -------------------------------\n",
    "\n",
    "        # set the discriminator gradients to zero\n",
    "        netD.zero_grad()\n",
    "        # load the image to GPU memory \n",
    "        real_cpu = data[0].to(device)\n",
    "        b_size = real_cpu.size(0)\n",
    "\n",
    "        # create the real labels for real images\n",
    "        label = torch.full((b_size,), real_label, dtype=torch.float, device=device)\n",
    "\n",
    "        # forward pass the image through discriminator \n",
    "        output = netD(real_cpu).view(-1)\n",
    "\n",
    "        # calculate the loss of discrimator w.r.to its label\n",
    "        errD_real = criterion(output, label)\n",
    "\n",
    "        # calculate and upgrade the gradients\n",
    "        errD_real.backward()\n",
    "\n",
    "        # calculate the mean loss of the batch \n",
    "        D_x = output.mean().item() #.item() function returns the variable to CPU memory\n",
    "\n",
    "        # generatre noise vector\n",
    "        noise = torch.randn(b_size, nz, 1, 1, device=device)\n",
    "\n",
    "        # forward pass the latent noise vector to generator\n",
    "        fake = netG(noise)\n",
    "        \n",
    "        # labels for fake images\n",
    "        label.fill_(fake_label)\n",
    "\n",
    "        #forward pass fake images to discriminator\n",
    "        output = netD(fake.detach()).view(-1)\n",
    "\n",
    "        # calculate loss w.r.to image labels\n",
    "        errD_fake = criterion(output, label)\n",
    "\n",
    "        # calculate and upgrade the gradients\n",
    "        errD_fake.backward()\n",
    "\n",
    "        # calculate the mean loss for the batch\n",
    "        D_G_z1 = output.mean().item()\n",
    "\n",
    "        # add loss for real batch and fake batch \n",
    "        errD = (errD_real + errD_fake)\n",
    "\n",
    "        # optimizer setup   \n",
    "        optimizerD.step()\n",
    "\n",
    "        #--------------------------------- Generator training ---------------------------------\n",
    "\n",
    "        # set generator gradients to zero \n",
    "        netG.zero_grad()\n",
    "\n",
    "        # create the labels for real images\n",
    "        label.fill_(real_label)  \n",
    "\n",
    "        # forward pass the generatred images to discriminator\n",
    "        output = netD(fake).view(-1)\n",
    " \n",
    "        # calculate loss for fake batch w.r.to real image labels\n",
    "        errG = criterion(output, label)\n",
    "\n",
    "        # calculate and update gradients\n",
    "        errG.backward()\n",
    "\n",
    "        #uncomment to plot the gradients flow for generator\n",
    "        plot_grad_flow(netG.named_parameters())\n",
    "\n",
    "        # calculate the mean loss for batch of fake images \n",
    "        D_G_z2 = output.mean().item()\n",
    "\n",
    "        # optimizer step\n",
    "        optimizerG.step()\n",
    "\n",
    "        # un comment to print used memeory of GPU during training \n",
    "        #print(torch.cuda.memory_allocated(device=device)/10**6,\"MB / \",torch.cuda.max_memory_allocated(device=device)/10**6,\"MB\")\n",
    "\n",
    "        # Save Losses for plotting later\n",
    "        G_losses.append(errG.item())\n",
    "        D_losses.append(errD.item())\n",
    "        \n",
    "        # print the losses to analyse during trainig\n",
    "        if i % 200  ==0:\n",
    "            print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\n",
    "                  % (epoch, num_epochs, i, len(dataloader),\n",
    "                     errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n",
    "            print(torch.cuda.memory_allocated(device=device)/10**6,\"MB / \",torch.cuda.max_memory_allocated(device=device)/10**6,\"MB\")\n",
    "        iters += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the generator and discriminator model \n",
    "torch.save(netG.state_dict(), 'netG.pt')\n",
    "torch.save(netD.state_dict(), 'netD.pt')\n",
    "torch.save(optimizerG.state_dict(), 'optimizerG.pt')\n",
    "torch.save(optimizerD.state_dict(), 'OptimizerD.pt')\n",
    "\n",
    "# the saved models can be loaded to resume training later "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
